# 实时

# 1.0 实时数据采集

1. ##### 在Master主节点使用Flume采集实时数据生成器25001端口的socket数据（实时数据生成器脚本为Master节点/data_log目录下的gen_ds_data_to_socket脚本，该脚本为Master节点本地部署且使用socket传输），将数据存入到Kafka的Topic中（Topic名称为ods_mall_log，分区数为4），使用Kafka自带的消费者消费ods_mall_log中的数据，查看Topic 中的前1 条数据的结果，将查看命令与结果完整的截图粘贴至客户端桌面【Release\模块B 提交结果.docx】中对应的任务序号下；

- ##### 创建 Flume 配置文件

  ```txt
  a1.channels = c1
  a1.sinks = k1
  a1.sources = s1
  
  a1.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSink
  a1.sinks.k1.topic = ods_mall_log
  a1.sinks.k1.brokerList = 192.168.45.10:9092
  
  a1.sources.s1.type = netcat
  a1.sources.s1.port = 25001
  a1.sources.s1.bind = 127.0.0.1
  
  a1.channels.c1.type = memory
  
  a1.sinks.k1.channels = c1
  a1.sources.s1.channels = c1
  ```

  

- 使用Flume命令启动Flume

  `flume-ng agent -f /配置文件 -c /配置文件所在目录 -n name`

- 启动实时数据生成器

  1. `cd /data_log`
  2. `./ gen_ds_data_to_socket`

- 查看前一条数据

  1. `cd /opt/module/kafka-2.4.1/bin`
  2. `kafka-console-consumer.sh --bootstrap-server 192.168.45.10:9092 --max-messages 1 --from-beginning --topic ods_mall_log`
     - --max-messages 1 消费一条数据
     - --from-beginning 从最早开始
     - --topic 指定主题



2. ##### 实时脚本启动后，在主节点进入到maxwell-1.29.0 的解压后目录下，配置相关文件并启动，读取主节点MySQL 数据的binlog 日志到Kafka 的Topic 中。使用Kafka 自带的消费者消费Topic 中的数据，查看Topic 中的前1 条数据的结果，将查看命令与结果完整的截图粘贴至客户端桌面【Release\模块B 提交结果.docx】中对应的任务序号下。



- 进入 maxwell 目录 conf文件

- 复制`config.properties.example` 并重命名为 config.properties

  `cp ./config.properties.example ./config.properties`